import streamlit as st
import requests
import json
from datetime import datetime
import time

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
st.set_page_config(
    page_title="My Personal Chat AI",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô
st.markdown("""
<style>
/* Import Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Global Styles */
.stApp {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    background-attachment: fixed;
}

/* Hide Streamlit elements */
#MainMenu {visibility: hidden;}
footer {visibility: hidden;}
.stDeployButton {display: none;}

/* Main header */
.main-header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 2rem;
    border-radius: 15px;
    margin-bottom: 2rem;
    text-align: center;
    color: white;
    box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    backdrop-filter: blur(10px);
}

.main-header h1 {
    font-size: 2.5rem;
    font-weight: 700;
    margin: 0;
    text-shadow: 0 2px 10px rgba(0,0,0,0.3);
}

.main-header p {
    font-size: 1.1rem;
    opacity: 0.9;
    margin: 0.5rem 0 0 0;
}

/* Chat container */
.chat-container {
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(20px);
    padding: 1.5rem;
    border-radius: 15px;
    margin: 1rem 0;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
    border: 1px solid rgba(255,255,255,0.2);
}

/* Message styles */
.user-message {
    background: linear-gradient(135deg, #007bff 0%, #0056b3 100%);
    color: white;
    padding: 0.75rem 1.25rem;
    border-radius: 20px;
    margin: 0.75rem 0;
    text-align: right;
    box-shadow: 0 4px 15px rgba(0,123,255,0.3);
    font-weight: 500;
}

.ai-message {
    background: linear-gradient(135deg, #28a745 0%, #1e7e34 100%);
    color: white;
    padding: 0.75rem 1.25rem;
    border-radius: 20px;
    margin: 0.75rem 0;
    box-shadow: 0 4px 15px rgba(40,167,69,0.3);
    font-weight: 500;
}

/* Sidebar styles */
.sidebar-section {
    background: rgba(255, 255, 255, 0.1);
    backdrop-filter: blur(10px);
    padding: 1.25rem;
    border-radius: 12px;
    margin: 1rem 0;
    border: 1px solid rgba(255,255,255,0.2);
}

/* Custom chat message styling */
.stChatMessage {
    background: rgba(255, 255, 255, 0.95) !important;
    backdrop-filter: blur(20px);
    border-radius: 15px !important;
    border: 1px solid rgba(255,255,255,0.2);
    box-shadow: 0 4px 20px rgba(0,0,0,0.1);
    margin: 1rem 0 !important;
}

.stChatMessage[data-testid="user-message"] {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
    color: white;
}

.stChatMessage[data-testid="assistant-message"] {
    background: rgba(255, 255, 255, 0.95) !important;
}

/* Input styling */
.stChatInput > div {
    background: rgba(255, 255, 255, 0.95);
    backdrop-filter: blur(20px);
    border-radius: 25px;
    border: 2px solid rgba(255,255,255,0.3);
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}

.stChatInput input {
    background: transparent !important;
    border: none !important;
    font-size: 1rem;
    padding: 1rem 1.5rem;
}

/* Button styling */
.stButton > button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 10px;
    padding: 0.6rem 1.2rem;
    font-weight: 500;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
}

.stButton > button:hover {
    transform: translateY(-2px);
    box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
}

/* Selectbox styling */
.stSelectbox > div > div {
    background: rgba(255, 255, 255, 0.9);
    border-radius: 10px;
    border: 1px solid rgba(255,255,255,0.3);
    backdrop-filter: blur(10px);
}

/* Slider styling */
.stSlider > div > div > div {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}

/* Metric styling */
.stMetric {
    background: rgba(255, 255, 255, 0.1);
    padding: 1rem;
    border-radius: 10px;
    backdrop-filter: blur(10px);
    border: 1px solid rgba(255,255,255,0.2);
}

/* Reasoning box for R1 */
.reasoning-container {
    background: rgba(255, 248, 220, 0.95);
    border: 2px solid #ffd700;
    border-radius: 12px;
    padding: 1rem;
    margin: 1rem 0;
    backdrop-filter: blur(10px);
}

.reasoning-header {
    color: #b8860b;
    font-weight: 600;
    font-size: 0.9rem;
    margin-bottom: 0.5rem;
    display: flex;
    align-items: center;
    gap: 0.5rem;
}

.reasoning-content {
    background: rgba(255, 255, 255, 0.8);
    padding: 0.75rem;
    border-radius: 8px;
    font-family: 'Monaco', 'Consolas', monospace;
    font-size: 0.85rem;
    color: #444;
    white-space: pre-wrap;
    max-height: 300px;
    overflow-y: auto;
}

/* Footer styling */
.footer {
    text-align: center;
    color: rgba(255,255,255,0.8);
    font-size: 14px;
    margin-top: 2rem;
    padding: 1rem;
    background: rgba(255,255,255,0.1);
    border-radius: 10px;
    backdrop-filter: blur(10px);
}

/* Responsive design */
@media (max-width: 768px) {
    .main-header h1 {
        font-size: 2rem;
    }
    
    .main-header {
        padding: 1.5rem;
    }
    
    .chat-container {
        padding: 1rem;
    }
}

/* Animation */
@keyframes fadeIn {
    from { opacity: 0; transform: translateY(20px); }
    to { opacity: 1; transform: translateY(0); }
}

.stChatMessage {
    animation: fadeIn 0.5s ease-out;
}
</style>
""", unsafe_allow_html=True)

# ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å
st.markdown("""
<div class="main-header">
    <h1>ü§ñ My Personal Chat AI</h1>
    <p>Powered by DeepSeek API with Advanced Reasoning</p>
</div>
""", unsafe_allow_html=True)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á session state ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "api_calls_count" not in st.session_state:
    st.session_state.api_calls_count = 0

if "current_model" not in st.session_state:
    st.session_state.current_model = "deepseek-chat"

if "show_reasoning" not in st.session_state:
    st.session_state.show_reasoning = False

# Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤
with st.sidebar:
    st.markdown("### ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI
    model_options = {
        "deepseek-chat": "üí¨ DeepSeek Chat",
        "deepseek-coder": "üíª DeepSeek Coder",
        "deepseek-reasoner": "üß† DeepSeek R1 (Reasoning)"
    }
    
    selected_model = st.selectbox(
        "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• AI:",
        options=list(model_options.keys()),
        format_func=lambda x: model_options[x],
        index=0
    )
    st.session_state.current_model = selected_model
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model_descriptions = {
        "deepseek-chat": "üí¨ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ",
        "deepseek-coder": "üíª ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î", 
        "deepseek-reasoner": "üß† ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô"
    }
    st.info(model_descriptions[selected_model])
    
    # ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DeepSeek R1
    if selected_model == "deepseek-reasoner":
        st.session_state.show_reasoning = st.checkbox(
            "üîç ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î (Chain of Thought)",
            value=st.session_state.show_reasoning,
            help="‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ç‡∏≠‡∏á AI"
        )
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå AI
    st.markdown("#### üéõÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ AI")
    
    # ‡∏õ‡∏£‡∏±‡∏ö parameter range ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
    if selected_model == "deepseek-reasoner":
        temperature = st.slider("Temperature (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå)", 0.0, 1.0, 0.3, 0.1,
                               help="R1 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥")
        max_tokens = st.slider("Max Tokens (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö)", 1000, 8000, 4000, 500,
                              help="R1 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tokens ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•")
    else:
        temperature = st.slider("Temperature (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå)", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("Max Tokens (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö)", 100, 2000, 1000, 100)
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏©‡∏≤
    st.markdown("#### üåê ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏©‡∏≤")
    language_options = {
        "auto": "‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥",
        "th": "‡πÑ‡∏ó‡∏¢",
        "en": "English",
        "zh": "‰∏≠Êñá",
        "ja": "Êó•Êú¨Ë™û"
    }
    
    selected_language = st.selectbox(
        "‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö:",
        options=list(language_options.keys()),
        format_func=lambda x: language_options[x]
    )
    
    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    st.markdown("#### üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("API Calls", st.session_state.api_calls_count)
    with col2:
        st.metric("Messages", len(st.session_state.messages))
    
    st.markdown("---")
    
    # ‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("üì• Export Chat", use_container_width=True):
            if st.session_state.messages:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö export
                export_text = f"Chat Export - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                export_text += "="*50 + "\n\n"
                
                for msg in st.session_state.messages:
                    role = "‡∏Ñ‡∏∏‡∏ì" if msg["role"] == "user" else "AI"
                    export_text += f"{role}: {msg['content']}\n\n"
                
                st.download_button(
                    label="üíæ ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î",
                    data=export_text,
                    file_name=f"chat_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain"
                )
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
    if st.session_state.current_model == "deepseek-reasoner":
        st.markdown("---")
        st.markdown("#### üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1:")
        examples = [
            "‡πÅ‡∏Å‡πâ‡∏™‡∏°‡∏Å‡∏≤‡∏£: x¬≤ + 5x - 6 = 0",
            "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢",
            "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢",
            "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Python vs JavaScript"
        ]
        
        for i, example in enumerate(examples):
            if st.button(f"üí° {example[:25]}...", key=f"example_{i}", use_container_width=True):
                st.session_state.example_question = example
    
    # ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
    st.markdown("---")
    st.markdown("### üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥")
    st.markdown("""
    - **üí¨ DeepSeek Chat**: ‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    - **üíª DeepSeek Coder**: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î, debug
    - **üß† DeepSeek R1**: ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
    - **‡∏õ‡∏£‡∏±‡∏ö Temperature**: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå
    - **Export ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤**: ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    """)

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
st.markdown("### üí¨ ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á container ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö chat
chat_container = st.container()

with chat_container:
    for i, message in enumerate(st.session_state.messages):
        if message["role"] == "user":
            with st.chat_message("user", avatar="üë§"):
                st.markdown(f"**‡∏Ñ‡∏∏‡∏ì:** {message['content']}")
        else:
            with st.chat_message("assistant", avatar="ü§ñ"):
                content = message['content']
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
                if (st.session_state.current_model == "deepseek-reasoner" and 
                    st.session_state.show_reasoning and 
                    "<think>" in content and "</think>" in content):
                    
                    start_idx = content.find("<think>") + 7
                    end_idx = content.find("</think>")
                    reasoning_part = content[start_idx:end_idx].strip()
                    final_answer = content[end_idx+8:].strip()
                    
                    # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î
                    st.markdown("""
                    <div class="reasoning-container">
                        <div class="reasoning-header">üß† ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• (Chain of Thought)</div>
                        <div class="reasoning-content">{}</div>
                    </div>
                    """.format(reasoning_part), unsafe_allow_html=True)
                    
                    st.markdown(f"**AI:** {final_answer}")
                else:
                    st.markdown(f"**AI:** {content}")

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
def call_deepseek_api(messages, model, temperature, max_tokens, language):
    try:
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° system message ‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        system_messages = []
        if language != "auto":
            language_prompts = {
                "th": "‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
                "en": "Please respond in English",
                "zh": "ËØ∑Áî®‰∏≠ÊñáÂõûÁ≠î",
                "ja": "Êó•Êú¨Ë™û„ÅßÁ≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ"
            }
            if language in language_prompts:
                system_messages.append({
                    "role": "system", 
                    "content": language_prompts[language]
                })
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° system message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
        if model == "deepseek-reasoner":
            system_messages.append({
                "role": "system",
                "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏Ñ‡∏¥‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
            })
        
        # ‡∏£‡∏ß‡∏° system messages ‡∏Å‡∏±‡∏ö user messages
        all_messages = system_messages + [{"role": "user", "content": messages}]
        
        headers = {
            "Authorization": f"Bearer {st.secrets['DEEPSEEK_API_KEY']}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": all_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° reasoning parameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
        if model == "deepseek-reasoner":
            data["reasoning_effort"] = "medium"
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=60 if model == "deepseek-reasoner" else 30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1 ‡∏≠‡∏≤‡∏à‡∏°‡∏µ reasoning_content ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å
            if (model == "deepseek-reasoner" and 
                "reasoning_content" in result["choices"][0]["message"]):
                reasoning = result["choices"][0]["message"]["reasoning_content"]
                if reasoning:
                    content = f"<think>\n{reasoning}\n</think>\n\n{content}"
            
            return content
        else:
            return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: HTTP {response.status_code}"
            
    except requests.exceptions.Timeout:
        return "‚è∞ ‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ (R1 ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥) ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
    except requests.exceptions.ConnectionError:
        return "üåê ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÑ‡∏î‡πâ"
    except Exception as e:
        return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"

# ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
if prompt := st.chat_input("üí≠ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...", key="user_input"):
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    with st.chat_message("user", avatar="üë§"):
        st.markdown(f"**‡∏Ñ‡∏∏‡∏ì:** {prompt}")
    
    # ‡πÅ‡∏™‡∏î‡∏á loading spinner
    with st.chat_message("assistant", avatar="ü§ñ"):
        loading_text = "üß† R1 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•..." if st.session_state.current_model == "deepseek-reasoner" else "ü§î AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."
        
        with st.spinner(loading_text):
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
            ai_response = call_deepseek_api(
                prompt,
                st.session_state.current_model,
                temperature,
                max_tokens,
                selected_language
            )
            
            # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
            st.session_state.api_calls_count += 1
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö R1
            if (st.session_state.current_model == "deepseek-reasoner" and 
                st.session_state.show_reasoning and 
                "<think>" in ai_response and "</think>" in ai_response):
                
                start_idx = ai_response.find("<think>") + 7
                end_idx = ai_response.find("</think>")
                reasoning_part = ai_response[start_idx:end_idx].strip()
                final_answer = ai_response[end_idx+8:].strip()
                
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î
                st.markdown("""
                <div class="reasoning-container">
                    <div class="reasoning-header">üß† ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• (Chain of Thought)</div>
                    <div class="reasoning-content">{}</div>
                </div>
                """.format(reasoning_part), unsafe_allow_html=True)
                
                st.markdown(f"**AI:** {final_answer}")
            else:
                # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
                st.markdown(f"**AI:** {ai_response}")
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö AI ‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
            st.session_state.messages.append({"role": "assistant", "content": ai_response})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å sidebar
if hasattr(st.session_state, 'example_question'):
    example_prompt = st.session_state.example_question
    del st.session_state.example_question
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏á‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "user", "content": example_prompt})
    
    with st.chat_message("assistant", avatar="ü§ñ"):
        with st.spinner("üß† R1 ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•..."):
            ai_response = call_deepseek_api(
                example_prompt,
                st.session_state.current_model,
                temperature,
                max_tokens,
                selected_language
            )
            
            st.session_state.api_calls_count += 1
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
    
    st.rerun()

# Footer
st.markdown("""
<div class="footer">
    üöÄ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢ Streamlit | ü§ñ Powered by DeepSeek API<br>
    Made with ‚ù§Ô∏è for personal use
</div>
""", unsafe_allow_html=True)

# ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ
if st.checkbox("üîß ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ"):
    st.json({
        "Streamlit Version": st.__version__,
        "Current Model": st.session_state.current_model,
        "Temperature": temperature,
        "Max Tokens": max_tokens,
        "Language": selected_language,
        "Total Messages": len(st.session_state.messages),
        "API Calls": st.session_state.api_calls_count,
        "Show Reasoning": st.session_state.show_reasoning
    })
